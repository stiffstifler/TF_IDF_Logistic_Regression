{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1zcPecglnIUn70htS2i+V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Task: Classifying textual data using a multi-label approach.\n","\n","Stack: TF-IDF, Logistic Regression\n","\n","Steps:\n","1. Data loading and normalization\n","2. label binarization and title vectorization\n","3. Splitting the data into training and test parts, according to 80/20 standard.\n","4. Model training\n","5. Model evaluation and results\n","\n","Result: Achieved 80% accuracy, which is 9% lower than the results obtained by Random Forest model.\n","\n","Difficulties:\n","- Owner label is equal to 0\n","- Insufficient handling of rare classes\n","\n","Solutions undertaken:\n","\n","- Added basic stop word cleaning\n","- Testing different variants of parameters max_iter, test_size.\n","- Decided to implement Random Forest model to compare results.\n","\n","Other possibilities for optimization:\n","- Vectorization\n","- Noise cleaning with NLTK tools\n","- Balancing classes to increase focus on rare metrics like Owner.\n","- Changing vectorization tool (e.g. Word2Vec)\n","- Changing the model (e.g. Random Forest, MLN, BERT)"],"metadata":{"id":"7MFjKDp2RiFI"}},{"cell_type":"code","source":["My results:\n","\n","Classification Report:\n","                              precision    recall  f1-score   support\n","\n","                       Owner       0.00      0.00      0.00         2\n","              Vice President       0.91      0.75      0.82        67\n","                    Director       0.94      0.87      0.90        97\n","Individual Contributor/Staff       0.93      0.97      0.95       226\n","                     Manager       0.85      0.34      0.49        32\n","               Chief Officer       0.88      0.17      0.29        40\n","\n","                   micro avg       0.93      0.80      0.86       464\n","                   macro avg       0.75      0.52      0.58       464\n","                weighted avg       0.91      0.80      0.83       464\n","                 samples avg       0.83      0.82      0.82       464\n","\n","Total model accuracy:\n","0.8013392857142857"],"metadata":{"id":"Af4lgf7nPrpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_4oGO6YsPz3s"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pv9EQVOWz8sr","executionInfo":{"status":"ok","timestamp":1736932495090,"user_tz":-60,"elapsed":2236,"user":{"displayName":"Stiff Stifler","userId":"02148404630227853373"}},"outputId":"d8d0f0ee-572e-490d-bd9b-f5da861b0ad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Classification Report:\n","                              precision    recall  f1-score   support\n","\n","                       Owner       0.00      0.00      0.00         2\n","              Vice President       0.91      0.75      0.82        67\n","                    Director       0.94      0.87      0.90        97\n","Individual Contributor/Staff       0.93      0.97      0.95       226\n","                     Manager       0.85      0.34      0.49        32\n","               Chief Officer       0.88      0.17      0.29        40\n","\n","                   micro avg       0.93      0.80      0.86       464\n","                   macro avg       0.75      0.52      0.58       464\n","                weighted avg       0.91      0.80      0.83       464\n","                 samples avg       0.83      0.82      0.82       464\n","\n","Total model accuracy:\n","0.8013392857142857\n"]}],"source":["from google.colab import drive\n","import re\n","import pandas as pandas\n","import numpy as numpy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","\n","\n","# 1. Data pre-processing\n","\n","# Dataset from Google Drive\n","drive.mount(\"/content/drive\")\n","data_path = \"/content/drive/My Drive/dataset_path/dataset_name.xlsx\"\n","data_frame = pandas.read_excel(data_path, sheet_name=\"in\")\n","\n","# Label merge\n","def preprocessing_labels(line):\n","    labels = [line[\"Column 1\"], line[\"Column 2\"], line[\"Column 3\"], line[\"Column 4\"]]\n","    result = []\n","    for label in labels:\n","      if pandas.notnull(label):\n","        result.append(label)\n","    return result\n","\n","data_frame[\"Labels\"] = data_frame.apply(preprocessing_labels, axis=1)\n","\n","# Text cleanup\n","def text_cleaner(text):\n","    text = text.lower() # Register\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Wildcards\n","    words = text.split() # Tokenization\n","\n","    stop_words = {\"at\", \"and\", \"to\", \"of\", \"in\", \"the\", \"a\", \"an\", \"for\", \"on\", \"with\", \"by\"} # Noise\n","    filtered_words = []\n","    for word in words:\n","        if word not in stop_words:\n","            filtered_words.append(word)\n","    cleaned_text = \" \".join(filtered_words)\n","\n","    return cleaned_text\n","\n","data_frame[\"Title\"] = data_frame[\"Title\"].apply(text_cleaner)\n","\n","\n","# 2. TF-IDF vectorization\n","vector = TfidfVectorizer(max_features=5000)\n","X_titles = vector.fit_transform(data_frame[\"Title\"]) # Title matrix\n","\n","# Labels in multi-label\n","all_labels = []\n","for labels in data_frame[\"Labels\"]:\n","    for label in labels:\n","        all_labels.append(label)\n","\n","unique_labels = list(set(all_labels))  # Remove duplicates\n","\n","# Converting text labels to numbers\n","label_binarizer = {}\n","for i, label in enumerate(unique_labels):\n","    label_binarizer[label] = i\n","\n","def encode_labels(labels):\n","    encoded = numpy.zeros(len(unique_labels))\n","\n","    for label in labels:\n","        encoded[label_binarizer[label]] = 1\n","\n","    return encoded\n","\n","Y_labels = numpy.array(data_frame[\"Labels\"].apply(encode_labels).tolist()) # Labels matrix\n","\n","\n","# 3. 80/20 data split\n","X_train, X_test, Y_train, Y_test = train_test_split(X_titles, Y_labels, test_size=0.20, random_state=1)\n","\n","# 4. Training\n","model = MultiOutputClassifier(LogisticRegression(max_iter=100, random_state=1))\n","model.fit(X_train, Y_train)\n","\n","# 5. Result\n","Y_predct = model.predict(X_test)\n","\n","results = classification_report(Y_test, Y_predct, target_names=unique_labels, zero_division=0)\n","accuracy = accuracy_score(Y_test, Y_predct)\n","\n","# Print metrics\n","print(f\"Classification Report:\\n{results}\")\n","print(f\"Total model accuracy:\\n{accuracy}\")"]}]}